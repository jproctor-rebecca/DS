{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSPT6_LS_DS_414_Topic_Modeling_RJProctor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jproctor-rebecca/DS/blob/main/module4-topic-modeling/DSPT6_LS_DS_414_Topic_Modeling_RJProctor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOMBkFKn6uEK"
      },
      "source": [
        "# Topic Modeling (Prepare)\n",
        "\n",
        "On Monday we talked about summarizing your documents using just token counts. Today, we're going to learn about a much more sophisticated approach - learning 'topics' from documents. Topics are a latent structure. They are not directly observable in the data, but we know they're there by reading them.\n",
        "\n",
        "> **latent**: existing but not yet developed or manifest; hidden or concealed.\n",
        "\n",
        "## Use Cases\n",
        "Primary use case: what the hell are your documents about? Who might want to know that in industry - \n",
        "* Identifying common themes in customer reviews\n",
        "* Discovering the needle in a haystack \n",
        "* Monitoring communications (Email - State Department) \n",
        "\n",
        "## Learning Objectives\n",
        "*At the end of the lesson you should be able to:*\n",
        "* Part 0: Warm-Up\n",
        "* Part 1: Describe how an LDA Model works\n",
        "* Part 2: Estimate a LDA Model with Gensim\n",
        "* Part 3: Interpret LDA results & Select the appropriate number of topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH44cTLfe_2f",
        "outputId": "ea25c82e-7dd5-4af3-8041-be305730f6b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Dependencies for the week (instead of conda)\n",
        "!wget https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/main/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-29 17:27:59--  https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137 [text/plain]\n",
            "Saving to: ‘requirements.txt.1’\n",
            "\n",
            "requirements.txt.1  100%[===================>]     137  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-29 17:27:59 (3.57 MB/s) - ‘requirements.txt.1’ saved [137/137]\n",
            "\n",
            "Requirement already satisfied: gensim==3.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: pyLDAvis==2.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: spacy==2.2.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: scikit-learn==0.22.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.22.2)\n",
            "Requirement already satisfied: seaborn==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.9.0)\n",
            "Requirement already satisfied: squarify==0.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.4.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.1.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (3.6.4)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.35.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (2.7.1)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.17.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (50.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (7.3.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 9)) (2.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (20.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (8.5.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.9.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3->-r requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 7)) (19.0.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 7)) (4.6.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 3)) (3.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag3QmLgYfAG8",
        "outputId": "a0f5148d-1502-417e-a80d-1f8dec53537d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg  # Can do lg, takes awhile\n",
        "# Also on Colab, need to restart runtime after this step!"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.3.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sh_nVrje_Ub",
        "outputId": "91f557d0-3da4-4755-edac-c7e4962dde2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.8.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZnPKCQ5ki-7"
      },
      "source": [
        "!pip install pandarallel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxH0cRGX6uEL"
      },
      "source": [
        "# Part 0: Warm-Up\n",
        "How do we do a grid search? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFjplZ7r3Lyk"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnyNg8sQ6uEL"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim import models\n",
        "\n",
        "# Download spacy model\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boKWVkRz6uEO"
      },
      "source": [
        "data = fetch_20newsgroups()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPpukADi7Ofv",
        "outputId": "4dbbd3d3-4918-4a89-dac1-8dc9af0ab3cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data.keys()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Jhb3j-7_e2",
        "outputId": "015c126a-bf39-4ea7-ef7d-8a46a6febd8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data['target_names']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbVoIoMm8HbR",
        "outputId": "556fa505-7d69-4f20-d8a1-5f9973ed98f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "data['data'][1000]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\nSubject: Diamond SS24X, Win 3.1, Mouse cursor\\nOrganization: National Library of Medicine\\nLines: 10\\n\\n\\nAnybody seen mouse cursor distortion running the Diamond 1024x768x256 driver?\\nSorry, don't know the version of the driver (no indication in the menus) but it's a recently\\ndelivered Gateway system.  Am going to try the latest drivers from Diamond BBS but wondered\\nif anyone else had seen this.\\n\\npost or email\\n\\n--Don Lindbergh\\ndabl2@lhc.nlm.nih.gov\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-39zg9a6uEQ"
      },
      "source": [
        "### GridSearch on Just Classifier\n",
        "* Fit the vectorizer and prepare BEFORE it goes into the gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q3NmHEo6uEQ"
      },
      "source": [
        "v1 = TfidfVectorizer()\n",
        "X_train = v1.fit_transform(data['data'])\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv46R5azLAWl"
      },
      "source": [
        "y_train = data['target']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiTYZMnXKCGe",
        "outputId": "eb76dabb-051d-4894-c225-3d4ddc8fb69b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<11314x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 1787565 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxu7V-6QKFej",
        "outputId": "46018c1e-56e5-4f52-9f67-6eb25d0b3ea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# same as todense()\n",
        "X_train.toarray()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX3a4nUI6uES"
      },
      "source": [
        "p1 = {\n",
        "    'n_estimators':[10,20],\n",
        "    'max_depth': [None, 7]\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U3dtJi-KczP",
        "outputId": "83c89357-c7ee-4704-c067-3cb4e594cb7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 130107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OfA7KNQ6uEU",
        "outputId": "ad6a1a6c-2a8f-45bc-dd99-ef9d47f22a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = RandomForestClassifier()\n",
        "gs1 = GridSearchCV(clf, p1, cv=5,n_jobs=-1, verbose=1)\n",
        "gs1.fit(X_train, y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators=100, n_jobs=None,\n",
              "                                              oob_score=False,\n",
              "                                              random_state=None, verbose=0,\n",
              "                                              warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'max_depth': [None, 7], 'n_estimators': [10, 20]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMq_hw5W6uEX"
      },
      "source": [
        "#gs1.predict([\"Sample text\"]); if we include the vectorizer in the pipe, \n",
        "# we can input raw text\n",
        "# to avoid error we need to transform text outside GridSearch obj"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDkMaXvG83Zt",
        "outputId": "55871180-d98f-4239-d5e7-232feea0ba02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_sample = v1.transform([\"Sample text\"])\n",
        "test_sample.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 130107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLgsZtKJ8o01",
        "outputId": "a440bbcd-bc71-4665-fc2e-5be80c72c8c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pred = gs1.predict(test_sample)\n",
        "pred"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLGkB8Vz9Otq",
        "outputId": "f7cdebd7-26a9-4262-e8e8-fa24311c960d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data['target_names'][pred[0]]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'comp.os.ms-windows.misc'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdVTuXKl6uEZ"
      },
      "source": [
        "### GridSearch with BOTH the Vectoizer & Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUeC6tgpMslB"
      },
      "source": [
        "#RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "#                       criterion='gini', max_depth=15, max_features=500,\n",
        "#                       max_leaf_nodes=None, max_samples=None,\n",
        "#                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#                       min_samples_leaf=1, min_samples_split=2,\n",
        "#                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "#                       n_jobs=None, oob_score=False, random_state=None,\n",
        "#                       verbose=0, warm_start=False)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97TjUtoI6uEZ",
        "outputId": "abcd1a50-8acd-45a1-e798-872264f7f98b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "# 1. Create a pipeline with a vectorize and a classifier\n",
        "# Create Pipeline Components\n",
        "# create vectorizor\n",
        "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "# create classifier\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "pipe = Pipeline([\n",
        "                 ('vect', vect), \n",
        "                 ('clf', rfc)\n",
        "               ])\n",
        "\n",
        "parameters = {\n",
        "    'vect__max_features': (1000,5000),\n",
        "    'clf__max_depth': (15, 20),\n",
        "    'clf__n_estimators':(100, 200),\n",
        "}\n",
        "\n",
        "# 2. Use Grid Search to optimize the entire pipeline\n",
        "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=1)\n",
        "grid_search.fit(data['data'], y_train)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 2),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words='english',\n",
              "                                                        strip...\n",
              "                                                               min_samples_split=2,\n",
              "                                                               min_weight_fraction_leaf=0.0,\n",
              "                                                               n_estimators=100,\n",
              "                                                               n_jobs=None,\n",
              "                                                               oob_score=False,\n",
              "                                                               random_state=None,\n",
              "                                                               verbose=0,\n",
              "                                                               warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=1,\n",
              "             param_grid={'clf__max_depth': (15, 20),\n",
              "                         'clf__n_estimators': (100, 200),\n",
              "                         'vect__max_features': (1000, 5000)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdIE67Us6uEc"
      },
      "source": [
        "pred = grid_search.predict([\"Sample text\"])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhcb5G0W-Dch",
        "outputId": "3877bb68-e0cb-41a9-ffb2-e9a84af40c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data['target_names'][pred[0]]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sci.electronics'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2R2ACd36uEd"
      },
      "source": [
        "Advantages to using GS with the Pipe:\n",
        "* Allows us to make predictions on raw text increasing reproducibility. :)\n",
        "* Allows us to tune the parameters of the vectorizer along side the classifier. :D "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZjG6U86uEe"
      },
      "source": [
        "# Part 1: Describe how an LDA Model works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMzU-XOM3LzW"
      },
      "source": [
        "[Your Guide to Latent Dirichlet Allocation](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)\n",
        "\n",
        "[LDA Topic Modeling](https://lettier.com/projects/lda-topic-modeling/)\n",
        "\n",
        "[Topic Modeling with Gensim](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaMsy1XAGLxc",
        "outputId": "295168ad-5478-4859-9d1a-392ed9a40e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "df = pd.DataFrame({\n",
        "    'content': data['data'],\n",
        "    'target': data['target'],\n",
        "    'target_names': [data['target_names'][i] for i in data['target']]\n",
        "})\n",
        "df"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "      <td>1</td>\n",
              "      <td>comp.graphics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11309</th>\n",
              "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
              "      <td>13</td>\n",
              "      <td>sci.med</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11310</th>\n",
              "      <td>From: ebodin@pearl.tufts.edu\\nSubject: Screen ...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11311</th>\n",
              "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
              "      <td>3</td>\n",
              "      <td>comp.sys.ibm.pc.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11312</th>\n",
              "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
              "      <td>1</td>\n",
              "      <td>comp.graphics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11313</th>\n",
              "      <td>From: gunning@cco.caltech.edu (Kevin J. Gunnin...</td>\n",
              "      <td>8</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11314 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content  ...              target_names\n",
              "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...  ...                 rec.autos\n",
              "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...  ...     comp.sys.mac.hardware\n",
              "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...  ...     comp.sys.mac.hardware\n",
              "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...  ...             comp.graphics\n",
              "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...  ...                 sci.space\n",
              "...                                                  ...  ...                       ...\n",
              "11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...  ...                   sci.med\n",
              "11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...  ...     comp.sys.mac.hardware\n",
              "11311  From: westes@netcom.com (Will Estes)\\nSubject:...  ...  comp.sys.ibm.pc.hardware\n",
              "11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...  ...             comp.graphics\n",
              "11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...  ...           rec.motorcycles\n",
              "\n",
              "[11314 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na2bkOcFGter",
        "outputId": "70b76603-7be3-4a76-b408-457c1df924e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For reference on regex: https://docs.python.org/3/library/re.html\n",
        "\n",
        "# From 'content' column: \n",
        "# 1. Remove whitespace \n",
        "df['content'] = df['content'].apply(lambda text: text.strip())\n",
        "# 2. Remove Emails\n",
        "df['content'] = df['content'].apply(lambda text: re.sub('From: \\S+@\\S+', '', text))\n",
        "# 3. Remove new line characters\n",
        "df['content'] = df['content'].apply(lambda text: re.sub('\\\\n', '', text))\n",
        "# 4. Remove non-alphanumeric characters\n",
        "df['content'] = df['content'].apply(lambda text: re.sub('[^0-9 a-zA-Z]+', '', text))\n",
        "\n",
        "df['content'] = df['content'].apply(lambda text: text.strip())\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<input>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "<ipython-input-28-52aa6dc6234b>:7: DeprecationWarning: invalid escape sequence \\S\n",
            "  df['content'] = df['content'].apply(lambda text: re.sub('From: \\S+@\\S+', '', text))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UsWQenhWK88",
        "outputId": "4ed523d3-bf15-4d03-cadd-a8d569fbb710",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df['content'].head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    wheres my thingSubject WHAT car is thisNntpPos...\n",
              "1    Guy KuoSubject SI Clock Poll  Final CallSummar...\n",
              "2    Thomas E WillisSubject PB questionsOrganizatio...\n",
              "3    Joe GreenSubject Re Weitek P9000 Organization ...\n",
              "4    Jonathan McDowellSubject Re Shuttle Launch Que...\n",
              "Name: content, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQKxLOGuWz3q",
        "outputId": "b05484eb-639f-4a5a-82ff-364f19081f26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df['content'] = df['content'].apply(lambda text: text.strip())\n",
        "df['content'].head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    wheres my thingSubject WHAT car is thisNntpPos...\n",
              "1    Guy KuoSubject SI Clock Poll  Final CallSummar...\n",
              "2    Thomas E WillisSubject PB questionsOrganizatio...\n",
              "3    Joe GreenSubject Re Weitek P9000 Organization ...\n",
              "4    Jonathan McDowellSubject Re Shuttle Launch Que...\n",
              "Name: content, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2prKILFo3Lzg",
        "outputId": "f70c9399-407b-4a03-b934-006bbd7604b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandarallel in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from pandarallel) (0.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSW10Cb33Lzk"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EBPQXqEKE9P"
      },
      "source": [
        "# Create 'lemmas' column\n",
        "def tokenize(doc):\n",
        "  return [token, lemma_ for token in nlp(x) if (token.is_stop != True) and (token.is_punct !=True)]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP5OYI6XZhtu"
      },
      "source": [
        "TfidVectorizer(tokenizer=tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY51ttbOZ36o"
      },
      "source": [
        "df['lemmas'] = df['lemmas'].parallel_apply(lambda x: (token.lemma_ for  token in nlp(x) if (token.is_stop !=True) and (token.is_punct !=True)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvUZZFbNadXE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yvsyIpvKBlw"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPuCuK0z3Lzs"
      },
      "source": [
        "### The two main inputs to the LDA topic model are the dictionary (id2word) and the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1klqRpqtJxWc"
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(df['lemmas'] )\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in df['lemmas']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArZPxcP5LH1J"
      },
      "source": [
        "id2word[200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IIBnI2e3Lzw"
      },
      "source": [
        "df['content'][5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGFG9E2j3Lzz"
      },
      "source": [
        "corpus[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB6wox963Lz2"
      },
      "source": [
        "id2word[252]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk3g75XX3Lz4"
      },
      "source": [
        "id2word[276]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K2jWxHJLOzK"
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[(id2word[word_id], word_count) for word_id, word_count in corpus[5]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le-XzI923Lz8"
      },
      "source": [
        "# Part 2: Estimate a LDA Model with Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlNG4bSI3Lz8"
      },
      "source": [
        " ### Train an LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fasvjf0VLQ2a"
      },
      "source": [
        "# %%time\n",
        "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "#                                            id2word=id2word,\n",
        "#                                            num_topics=20, \n",
        "#                                            chunksize=100,\n",
        "#                                            passes=10,\n",
        "#                                            per_word_topics=True)\n",
        "\n",
        "# # https://radimrehurek.com/gensim/models/ldamodel.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49CabmIj3Lz_"
      },
      "source": [
        "# lda_model.save('lda_model.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDgUshRE3L0C"
      },
      "source": [
        "# %%time\n",
        "# lda_multicore = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "#                                                         id2word=id2word,\n",
        "#                                                         num_topics=20, \n",
        "#                                                         chunksize=100,\n",
        "#                                                         passes=10,\n",
        "#                                                         per_word_topics=True,\n",
        "#                                                         workers=12)\n",
        "\n",
        "# # https://radimrehurek.com/gensim/models/ldamulticore.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgb9AaPq3L0E"
      },
      "source": [
        "# lda_multicore.save('lda_multicore.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLxvJPoK3L0G"
      },
      "source": [
        "\n",
        "lda_multicore =  models.LdaModel.load('lda_multicore.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDH3tzx13L0I"
      },
      "source": [
        "### View the topics in LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JawR8yscLVNS"
      },
      "source": [
        "pprint(lda_multicore.print_topics())\n",
        "doc_lda = lda_multicore[corpus]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKEP0bIC3L0K"
      },
      "source": [
        "### What is topic Perplexity?\n",
        "Perplexity is a statistical measure of how well a probability model predicts a sample. As applied to LDA, for a given value of , you estimate the LDA model. Then given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents.\n",
        "\n",
        "### What is topic coherence?\n",
        "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference.\n",
        "A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is “the game is a team sport”, “the game is played with a ball”, “the game demands great physical efforts”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCjhr8k4LXSy"
      },
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_multicore.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_multicore, \n",
        "                                     texts=df['lemmas'], \n",
        "                                     dictionary=id2word, \n",
        "                                     coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UvfgYt93L0X"
      },
      "source": [
        "# Part 3: Interpret LDA results & Select the appropriate number of topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYXi480VLaHK"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_multicore, corpus, id2word)\n",
        "pyLDAvis.display(vis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjtXk8J3LaXC"
      },
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                        id2word=id2word,\n",
        "                                                        num_topics=num_topics, \n",
        "                                                        chunksize=100,\n",
        "                                                        passes=10,\n",
        "                                                        per_word_topics=True,\n",
        "                                                        workers=12)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRA9EjvQ3L0c"
      },
      "source": [
        "# %%time\n",
        "# model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df['lemmas'], start=2, limit=40, step=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e9X-Ql-3L0e"
      },
      "source": [
        "coherence_values = [0.5054, 0.5332, 0.5452, 0.564, 0.5678, 0.5518, 0.519]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybEBYQNF3L0g"
      },
      "source": [
        "limit=40; start=2; step=6;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmvQ2zKZ3L0i"
      },
      "source": [
        "# Print the coherence scores\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yol5vG3R3L0k"
      },
      "source": [
        "# Select the model and print the topics\n",
        "#optimal_model = model_list[4]\n",
        "optimal_model =  models.LdaModel.load('optimal_model.model')\n",
        "model_topics = optimal_model.show_topics(formatted=False)\n",
        "pprint(optimal_model.print_topics(num_words=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HwVEUDI3L0m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}